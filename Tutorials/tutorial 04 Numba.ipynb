{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2y4BT320afRz"
   },
   "source": [
    "# CDP - 236370\n",
    "# Tutorial 4 - Numba jit & cuda\n",
    "\n",
    "Written by Ido Hakimi, Qasem Sayah, and Anny Firer.\n",
    "<br>\n",
    "Adapted from https://nyu-cds.github.io/python-numba/ & https://numba.pydata.org/numba-doc/dev/user/5minguide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "IsoApQBXaor8",
    "outputId": "93113263-10d3-457c-89e1-54875aac9e98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5c3X40wafR1"
   },
   "source": [
    "Decorators are a way to uniformly modify functions in a particular way. You can think of them as functions that take functions as input and produce a function as output. See the Python reference documentation for a detailed discussion (https://docs.python.org/3/reference/compound_stmts.html#function-definitions).\n",
    "\n",
    "A function definition may be wrapped by one or more decorator expressions. Decorator expressions are evaluated when the function is defined, in the scope that contains the function definition. The result must be a callable, which is invoked with the function object as the only argument. The returned value is bound to the function name instead of the function object. Multiple decorators are applied in nested fashion. For example, the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4iLGRsxKGDP"
   },
   "outputs": [],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper(string):\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func(string)\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_whee(string):\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8s8oGkrWKZE2"
   },
   "outputs": [],
   "source": [
    "say_whee(\"Whee!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4klQDZ1Urbqh"
   },
   "source": [
    "## Just-In-Time (JIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJct_ZewrhAx"
   },
   "source": [
    "JIT is a way of executing computer code that involves compilation during execution of a program – at run time – rather than prior to execution. Most often, this consists of source code or more commonly bytecode translation to machine code, which is then executed directly. A system implementing a JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code. \n",
    "\n",
    "JIT compilation is a combination of the two traditional approaches to translation to machine code – ahead-of-time compilation (AOT), and interpretation – and combines some advantages and drawbacks of both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aD3xYnVDOfQN"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def f(x, y):\n",
    "    # A somewhat trivial example\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WORX1r1UW8-i"
   },
   "source": [
    "## Explicit Parallel Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrqdbIbZXAOY"
   },
   "source": [
    "Another feature of this code transformation pass is support for explicit parallel loops. One can use Numba’s prange instead of range to specify that a loop can be parallelized. The user is required to make sure that the loop does not have cross iteration dependencies except for supported reductions.\n",
    "\n",
    "A reduction is inferred automatically if a variable is updated by a binary function/operator using its previous value in the loop body. The initial value of the reduction is inferred automatically for += and *= operators. For other functions/operators, the reduction variable should hold the identity value right before entering the prange loop. Reductions in this manner are supported for scalars and for arrays of arbitrary dimensions.\n",
    "\n",
    "The example below demonstrates a parallel loop with a reduction (A is a one-dimensional Numpy array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrB8QF9VXFwq"
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "@njit(parallel=True)\n",
    "def prange_test(A):\n",
    "    s = 0\n",
    "    for i in prange(A.shape[0]):\n",
    "        s += A[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4V2QloN9afR2"
   },
   "source": [
    "## Numba Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngA1gg6vrjt5"
   },
   "source": [
    "Numba’s central feature is the `numba.jit()` decoration. Using this decorator, it is possible to mark a function for optimization by Numba’s JIT compiler. Various invocation modes trigger differing compilation options and behaviours.\n",
    "\n",
    "Numba has two compilation modes: *nopython mode* and *object mode*. The former produces much faster code, but has limitations that can force Numba to fall back to the latter. To prevent Numba from falling back, and instead raise an error, pass `nopython=True` as we do bellow.\n",
    "\n",
    "You can read about more arguments that can be passed to `numba.jit()` in the official documentation: http://numba.pydata.org/numba-doc/latest/user/jit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Euz6dwlwafR3",
    "outputId": "aa569283-ab1c-4b37-b27b-76dcf6c3901a"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))\n",
    "\n",
    "# NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (after compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0WdOHKJafR8"
   },
   "source": [
    "## Bubble Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIO4353NrrRo"
   },
   "source": [
    "Let’s see Numba in action. The following is a Python implementation of bubblesort for NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrVH99bfafR9"
   },
   "outputs": [],
   "source": [
    "def bubblesort(X):\n",
    "    N = len(X)\n",
    "    for end in range(N, 1, -1):\n",
    "        for i in range(end - 1):\n",
    "            cur = X[i]\n",
    "            if cur > X[i + 1]:\n",
    "                tmp = X[i]\n",
    "                X[i] = X[i + 1]\n",
    "                X[i + 1] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTxUmMItryLq"
   },
   "source": [
    "First we’ll create an array of sorted values and randomly shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ME12mErqafR_",
    "outputId": "198d47e2-e560-4885-b898-73871f4e27cb"
   },
   "outputs": [],
   "source": [
    "original = np.arange(0.0, 10.0, 0.01)\n",
    "original[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uTupuSrhafSC",
    "outputId": "dd6d0581-2a76-45e4-d2f1-2559d5944a83"
   },
   "outputs": [],
   "source": [
    "shuffled = original.copy()\n",
    "np.random.shuffle(shuffled)\n",
    "shuffled[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtfUSTm9r2s6"
   },
   "source": [
    "Next, create a copy and do a bubble sort on the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "blUZEPO6afSI",
    "outputId": "f9577f57-5385-4f0d-e18a-46b336e810c4"
   },
   "outputs": [],
   "source": [
    "arr_copy = shuffled.copy()\n",
    "bubblesort(arr_copy)\n",
    "arr_copy[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hg94TaDgafSK",
    "outputId": "4d9d11c7-c932-4591-aa11-3ace5c7127db"
   },
   "outputs": [],
   "source": [
    "np.array_equal(arr_copy, original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bT0zn6bEr86u"
   },
   "source": [
    "Now let’s time the execution. Note: we need to copy the array so we sort a random array each time as sorting an already sorted array is faster and so would distort our timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ShEahjZ_afSM",
    "outputId": "78ad58e3-ccc3-4193-9ebf-3edb60e7e505"
   },
   "outputs": [],
   "source": [
    "%timeit bubblesort(shuffled.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9MGWTzrr7ui"
   },
   "source": [
    "Ok, so we know how fast the pure Python implementation is. Now we will add the decorator to the function with the paramater nopython set to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44kR8-SyafSS"
   },
   "outputs": [],
   "source": [
    "fast_bubblesort = jit(nopython=True)(bubblesort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-QBjdD29afSU",
    "outputId": "5f927bcf-70f9-480d-da89-e7f9f675e66e"
   },
   "outputs": [],
   "source": [
    "fast_bubblesort(shuffled.copy())\n",
    "%timeit fast_bubblesort(shuffled.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6YuX0M-aafSX",
    "outputId": "dc1e8d0f-9e87-43ab-d06a-154eb83b2785"
   },
   "outputs": [],
   "source": [
    "%timeit np.sort(shuffled.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSF7FIVYsJt5"
   },
   "source": [
    "Using the decorator in this way will defer compilation until the first function execution, so the first execution will be significantly slower.\n",
    "\n",
    "Numba will infer the argument types at call time, and generate optimized code based on this information. Numba will also be able to compile separate specializations depending on the input types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "la4UTP8qafSa"
   },
   "source": [
    "## CUDA\n",
    "### Terminology\n",
    "Several important terms in the topic of CUDA programming are listed here:\n",
    "\n",
    "#### host\n",
    "the CPU\n",
    "#### device\n",
    "the GPU\n",
    "#### host memory\n",
    "the system main memory\n",
    "#### device memory\n",
    "onboard memory on a GPU card\n",
    "#### kernel\n",
    "a GPU function launched by the host and executed on the device\n",
    "#### device function\n",
    "a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKTFeN4SbXWH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# enabling CUDA simulation:\n",
    "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUsviUjSafSb"
   },
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFcFY6VuafSd"
   },
   "source": [
    "## Thread positioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HJ289nvafSe"
   },
   "source": [
    "When running a kernel, the kernel function’s code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for. More complex algorithms may define more complex responsibilities, but the underlying principle is the same.\n",
    "\n",
    "To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1-dimensional declarations of equivalent sizes, this doesn’t change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.\n",
    "\n",
    "One way is for the thread to determines its position in the grid and block and manually compute the corresponding array position:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQMut8z9lGwp"
   },
   "source": [
    "### Kernel declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQuJpKksafSf"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + bx * bw\n",
    "    if pos < io_array.size:  # Check array boundaries\n",
    "        io_array[pos] *= 2 # do the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93TwLqiVcxOG"
   },
   "source": [
    "### Kernel invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhLmRINZafSi"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Create the data array - usually initialized some other way\n",
    "data = numpy.ones(256)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "# Print the result\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDwIV_78afSm"
   },
   "source": [
    "The following special objects are provided by the CUDA backend for the sole purpose of knowing the geometry of the thread hierarchy and the position of the current thread within that geometry:\n",
    "* **numba.cuda.threadIdx** - The thread indices in the current thread block. For 1-dimensional blocks, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.blockDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "* **numba.cuda.blockDim** - The shape of the block of threads, as declared when instantiating the kernel. This value is the same for all threads in a given kernel, even if they belong to different blocks (i.e. each block is “full”).\n",
    "* **numba.cuda.blockIdx** - The block indices in the grid of threads launched a kernel. For a 1-dimensional grid, the index (given by the x attribute) is an integer spanning the range from 0 to numba.cuda.gridDim - 1. A similar rule exists for each dimension when more than one dimension is used.\n",
    "* **numba.cuda.gridDim** - The shape of the grid of blocks, i.e. the total number of blocks launched by this kernel invocation, as declared when instantiating the kernel.\n",
    "\n",
    "These objects can be 1-, 2- or 3-dimensional, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlpRj1yxafSn"
   },
   "source": [
    "## Absolute positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFQM7QCfafSp"
   },
   "source": [
    "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
    "* **numba.cuda.grid(ndim)** - Return the absolute position of the current thread in the entire grid of blocks. ndim should correspond to the number of dimensions declared when instantiating the kernel. If ndim is 1, a single integer is returned. If ndim is 2 or 3, a tuple of the given number of integers is returned.\n",
    "* **numba.cuda.gridsize(ndim)** - Return the absolute size (or shape) in threads of the entire grid of blocks. ndim has the same meaning as in grid() above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3CGlkUuafS8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "@cuda.jit\n",
    "def increment_a_2D_array(an_array):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
    "        an_array[x, y] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GKY-U9GafS9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "an_array = np.ones((64, 64))\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(an_array.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(an_array.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "increment_a_2D_array[blockspergrid, threadsperblock](an_array)\n",
    "an_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECqEOJZAgmQ3"
   },
   "source": [
    "## Data transfer\n",
    "Allocate and transfer a numpy ndarray or structured scalar to the device.\n",
    "* To copy host->device a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_pKf3Vjgui2"
   },
   "outputs": [],
   "source": [
    "ary = np.arange(10)\n",
    "d_ary = cuda.to_device(ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNIPTw-ehHm_"
   },
   "source": [
    "* To copy device->host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfzxw5clhFIS"
   },
   "outputs": [],
   "source": [
    "h_ary = d_ary.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-AtNNPIhUXU"
   },
   "source": [
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24rXg7xMhTwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "arr = np.arange(1000)\n",
    "d_arr = cuda.to_device(arr)\n",
    "\n",
    "my_kernel[100, 100](d_arr)\n",
    "\n",
    "result_array = d_arr.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbNB8wfWid0n"
   },
   "source": [
    "## Atomic Operations\n",
    "Numba provides access to some of the atomic operations supported in CUDA, in the numba.cuda.atomic class.\n",
    "\n",
    "including: add, compare_and_swap, max, min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-6ixFW-idJu"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def max_example(result, values):\n",
    "    \"\"\"Find the maximum value in values and store in result[0]\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    i = (bid * bdim) + tid\n",
    "    cuda.atomic.max(result, 0, values[i])\n",
    "\n",
    "\n",
    "arr = np.random.rand(16384)\n",
    "result = np.zeros(1, dtype=np.float64)\n",
    "\n",
    "max_example[256,64](result, arr)\n",
    "print(result[0]) # Found using cuda.atomic.max\n",
    "print(max(arr))  # Print max(arr) for comparision (should be equal!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "For maximum performance, a CUDA kernel needs to use shared memory for manual caching of data. CUDA JIT supports the use of cuda.shared.array(shape, dtype) for specifying an NumPy-array-like object inside a kernel.\n",
    "\n",
    "Note: shared memory is accessed per threadBlock only and is of limited amount (4-16KB hardware dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuda syncthreads\n",
    "Synchronize all threads in the same thread block. This function implements the same pattern as barriers in traditional multi-threaded programming: this function waits until all threads in the block call it, at which point it returns control to all its callers.\n",
    "\n",
    "Example: calculating the square of sum of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def sum_example(values,result):\n",
    "    tx = cuda.threadIdx.x\n",
    "    partial_sum = 0\n",
    "    for i in range(tx,values.size,32):\n",
    "        partial_sum += values[i]\n",
    "    cuda.atomic.add(result, 0, partial_sum)\n",
    "\n",
    "    cuda.syncthreads() # Wait until all threads finish\n",
    "    \n",
    "    if tx == 0:\n",
    "        result[0] **= 2\n",
    "\n",
    "arr = np.ones(32*4)\n",
    "result = np.zeros(1, dtype=np.int32)\n",
    "\n",
    "sum_example[1,32](arr, result)\n",
    "print(result[0]) # Found using cuda.atomic.add\n",
    "print(sum(arr)**2)  # Print result for comparision (should be equal!)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_5_numba.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
